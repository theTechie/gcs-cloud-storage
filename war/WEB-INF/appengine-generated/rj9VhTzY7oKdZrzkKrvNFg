1. GPU Speed : 
	GFLOPS (10^9) ; Giga Floating point operations per sec
	GILOPS (10^9) ; Giga Integer operations per sec

2. Processor Speed with full concurrency : (mapping 1 thread per core)  (2 experiments)
	Approach - 
		- Get number of cores using 'deviceQuery' and use that many threads for full concurrency

3. (Result-> 1 & 2) Compare measured GPU speed with theoritical compute speed and explain results.

4. Measure Read/Write memory bandwidth of GPU memory with different size messages (1B, 1KB, 1MB)
	Theoritical Memory Bandwidth : (GBPS); 
	(bit-interface / 8) * frequency * 2 => (128-bit / 8) * (900 Mhz, memory clock) * 2 = 28.8 GB/s

5. (Result-> 4) Compare measured memory bandwidth with theoritical memory bandwidth and explain results.
	Practical Memory bandwidth : (GBPS); 
	(N (number of operations) * 4 (bytes transferred per array) * 3 (no of read / write operations)) / milliseconds / 1e6 ) => 24.16 GB/s

6. Generic GPU Code
  
	- Read number of cores, threads per block and execute accordingly (deviceQuery)
	- nVidia vs AMD (CUDA vs OpenGL). how to make a switch ?
	- Use clock.cu (sample) to measure execution time across block threads


ALUs = Streaming Processors = CUDA Cores


Pening:
 1. GFLOPS (proper values)
 2. IFLOPS
 3. Processor speed with full concurrency (96 cores / threads; mapping 1 core per thread)
 4. R/W Memory Bandwidth with different size messages (1B, 1KB, 1MB)
